{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:03:33.963596Z",
     "start_time": "2025-08-25T14:03:33.954126Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wjbok\\\\Desktop\\\\BigTech'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1048a7fbe7e0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:04:03.410793Z",
     "start_time": "2025-08-25T14:04:02.376248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 대상 폴더: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train\n",
      "\n",
      "✅ 클래스별 객체 수 (내림차순)\n",
      "------------------------------\n",
      "   Class  Count\n",
      "0      1    958\n",
      "1      0    112\n",
      "2      5     59\n",
      "3      3     49\n",
      "4      4     36\n",
      "5      6     28\n",
      "6      2     22\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "BASE = r\"./object_detection_\"\n",
    "LBL_SRC = os.path.join(BASE,\"labels\",\"train\")\n",
    "\n",
    "\n",
    "def analyze_class_distribution(label_directory):\n",
    "    \"\"\"\n",
    "    지정된 디렉토리의 라벨 파일을 분석하여 클래스 분포를 표로 출력합니다.\n",
    "    \"\"\"\n",
    "    print(f\"분석 대상 폴더: {os.path.abspath(label_directory)}\")\n",
    "\n",
    "    # 1. 경로 존재 여부 확인\n",
    "    if not os.path.isdir(label_directory):\n",
    "        print(f\"❌ 오류: '{label_directory}' 폴더를 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "        return\n",
    "\n",
    "    # 2. 모든 클래스 ID 수집\n",
    "    class_ids = []\n",
    "    try:\n",
    "        label_files = [f for f in os.listdir(label_directory) if f.endswith(\".txt\")]\n",
    "        if not label_files:\n",
    "            print(f\"⚠️ 경고: '{label_directory}' 폴더에 라벨 파일(.txt)이 없습니다.\")\n",
    "            return\n",
    "\n",
    "        for filename in label_files:\n",
    "            filepath = os.path.join(label_directory, filename)\n",
    "            with open(filepath, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    if parts:\n",
    "                        try:\n",
    "                            class_id = int(parts[0])\n",
    "                            class_ids.append(class_id)\n",
    "                        except (ValueError, IndexError):\n",
    "                            continue # 숫자가 아니거나 빈 줄인 경우 무시\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류: 파일을 읽는 중 문제가 발생했습니다: {e}\")\n",
    "        return\n",
    "\n",
    "    if not class_ids:\n",
    "        print(\"분석할 클래스 데이터를 찾지 못했습니다.\")\n",
    "        return\n",
    "\n",
    "    # 3. 클래스별 개수 계산 및 표로 출력\n",
    "    class_counts = Counter(class_ids)\n",
    "    df = pd.DataFrame(class_counts.items(), columns=['Class', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n✅ 클래스별 객체 수 (내림차순)\")\n",
    "    print(\"-\" * 30)\n",
    "    print(df_sorted.to_string())\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 스크립트 실행\n",
    "if __name__ == '__main__':\n",
    "    analyze_class_distribution(LBL_SRC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85a583535fb4f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:05:33.004021Z",
     "start_time": "2025-08-25T14:05:02.170804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from albumentations) (1.16.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from albumentations) (6.0.1)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Downloading stringzilla-3.12.6-cp311-cp311-win_amd64.whl.metadata (81 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Downloading simsimd-6.5.1-cp311-cp311-win_amd64.whl.metadata (72 kB)\n",
      "Collecting numpy>=1.24.4 (from albumentations)\n",
      "  Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Downloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/38.9 MB 10.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.2/38.9 MB 11.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 6.8/38.9 MB 11.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 8.7/38.9 MB 10.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.0/38.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.4/38.9 MB 10.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.7/38.9 MB 10.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 17.8/38.9 MB 10.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.2/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 22.3/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.6/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 27.3/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.6/38.9 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.7/38.9 MB 10.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.3/38.9 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.9 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.9/38.9 MB 10.5 MB/s  0:00:03\n",
      "Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 9.8 MB/s  0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading simsimd-6.5.1-cp311-cp311-win_amd64.whl (94 kB)\n",
      "Downloading stringzilla-3.12.6-cp311-cp311-win_amd64.whl (79 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: stringzilla, simsimd, typing-inspection, pydantic-core, numpy, annotated-types, pydantic, opencv-python-headless, albucore, albumentations\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 1.26.4\n",
      "\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   -------------------------------- -------  8/10 [albucore]\n",
      "   ------------------------------------ ---  9/10 [albumentations]\n",
      "   ------------------------------------ ---  9/10 [albumentations]\n",
      "   ---------------------------------------- 10/10 [albumentations]\n",
      "\n",
      "Successfully installed albucore-0.0.24 albumentations-2.0.8 annotated-types-0.7.0 numpy-2.2.6 opencv-python-headless-4.12.0.88 pydantic-2.11.7 pydantic-core-2.33.2 simsimd-6.5.1 stringzilla-3.12.6 typing-inspection-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datumaro 1.10.0 requires numpy<2,>=1.23.4, but you have numpy 2.2.6 which is incompatible.\n",
      "mediapipe 0.10.14 requires protobuf<5,>=4.25.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "tensorflow-intel 2.14.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cedc29d60c70b381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:26:33.540758Z",
     "start_time": "2025-08-25T14:26:30.178522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 해상도를 분석합니다...\n",
      "\n",
      "✅ 전체 데이터셋의 해상도 분포\n",
      "-----------------------------------\n",
      "  Resolution  Count\n",
      "0    640x360    650\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- 설정 ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- 설정 ---\n",
    "\n",
    "SPLITS_TO_CHECK = [\"train\"]\n",
    "all_resolutions = []\n",
    "\n",
    "print(\"이미지 해상도를 분석합니다...\")\n",
    "for split in SPLITS_TO_CHECK:\n",
    "    img_dir = os.path.join(BASE, \"images\", split)\n",
    "    if not os.path.isdir(img_dir):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                img_path = os.path.join(img_dir, filename)\n",
    "                # OpenCV로 이미지 크기(너비, 높이) 읽기\n",
    "                image = cv2.imread(img_path)\n",
    "                height, width, _ = image.shape\n",
    "                all_resolutions.append(f\"{width}x{height}\")\n",
    "            except Exception as e:\n",
    "                print(f\"'{filename}' 파일 처리 중 오류 발생: {e}\")\n",
    "\n",
    "if not all_resolutions:\n",
    "    print(\"분석할 이미지가 없습니다.\")\n",
    "else:\n",
    "    resolution_counts = Counter(all_resolutions)\n",
    "    df = pd.DataFrame(resolution_counts.items(), columns=['Resolution', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n✅ 전체 데이터셋의 해상도 분포\")\n",
    "    print(\"-\" * 35)\n",
    "    print(df_sorted.to_string())\n",
    "    print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae6e04e07aae66b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:47:39.888648Z",
     "start_time": "2025-08-25T14:47:34.289999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\wjbok\\AppData\\Local\\Temp\\ipykernel_664\\205868126.py:13: UserWarning: Argument(s) 'value' are not valid for transform ShiftScaleRotate\n",
      "  A.ShiftScaleRotate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 증강 전 클래스별 이미지 수 ---\n",
      "  - 클래스 0: 111개 이미지\n",
      "  - 클래스 1: 366개 이미지\n",
      "  - 클래스 2: 22개 이미지\n",
      "  - 클래스 3: 49개 이미지\n",
      "  - 클래스 4: 36개 이미지\n",
      "  - 클래스 5: 59개 이미지\n",
      "  - 클래스 6: 28개 이미지\n",
      "-----------------------------------\n",
      "🔧 클래스 3 증강 시작 (목표: 100개, 생성: 51개)\n",
      "🔧 클래스 5 증강 시작 (목표: 100개, 생성: 41개)\n",
      "🔧 클래스 6 증강 시작 (목표: 100개, 생성: 72개)\n",
      "🔧 클래스 2 증강 시작 (목표: 100개, 생성: 78개)\n",
      "🔧 클래스 4 증강 시작 (목표: 100개, 생성: 64개)\n",
      "\n",
      "🎉 모든 소수 클래스 증강 작업이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "TARGET_IMAGE_COUNT=100\n",
    "\n",
    "IMG_DIR = os.path.join(BASE,\"images\",\"train\")\n",
    "LBL_DIR = os.path.join(BASE,\"labels\",\"train\")\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.0625,\n",
    "        scale_limit=0.1,\n",
    "        rotate_limit=15,\n",
    "        p=0.5,\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        value=0\n",
    "    ),\n",
    "    A.RandomResizedCrop(\n",
    "        size=(352, 640),\n",
    "        scale=(0.8, 1.0),\n",
    "        p=0.3\n",
    "    ),\n",
    "], bbox_params=A.BboxParams(\n",
    "        format='yolo',\n",
    "        label_fields=['class_labels'],\n",
    "        min_visibility=0.3\n",
    "))\n",
    "\n",
    "def augment_minority_classes():\n",
    "    \"\"\"소수 클래스를 분석하고 목표치에 도달할 때까지 증강합니다.\"\"\"\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"❌ 오류: 라벨 폴더 '{LBL_DIR}'를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    class_to_images = defaultdict(list)\n",
    "    for label_file in os.listdir(LBL_DIR):\n",
    "        if not label_file.endswith(\".txt\"): continue\n",
    "        image_name = os.path.splitext(label_file)[0]\n",
    "        with open(os.path.join(LBL_DIR, label_file), 'r') as f:\n",
    "            classes_in_image = set(int(line.split()[0]) for line in f if line.strip())\n",
    "            for class_id in classes_in_image:\n",
    "                class_to_images[class_id].append(image_name)\n",
    "\n",
    "    class_image_counts = {k: len(v) for k, v in class_to_images.items()}\n",
    "    print(\"--- 증강 전 클래스별 이미지 수 ---\")\n",
    "    for cid, count in sorted(class_image_counts.items()):\n",
    "        print(f\"  - 클래스 {cid}: {count}개 이미지\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for class_id, image_count in class_image_counts.items():\n",
    "        if image_count < TARGET_IMAGE_COUNT:\n",
    "            num_to_generate = TARGET_IMAGE_COUNT - image_count\n",
    "            source_images = class_to_images[class_id]\n",
    "            print(f\"🔧 클래스 {class_id} 증강 시작 (목표: {TARGET_IMAGE_COUNT}개, 생성: {num_to_generate}개)\")\n",
    "\n",
    "            for i in range(num_to_generate):\n",
    "                base_name = random.choice(source_images)\n",
    "                img_path, lbl_path = None, os.path.join(LBL_DIR, base_name + \".txt\")\n",
    "                for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    if os.path.exists(os.path.join(IMG_DIR, base_name + ext)):\n",
    "                        img_path = os.path.join(IMG_DIR, base_name + ext)\n",
    "                        break\n",
    "                if not img_path: continue\n",
    "\n",
    "                image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "                bboxes, class_labels = [], []\n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        class_labels.append(int(parts[0]))\n",
    "                        bboxes.append([float(p) for p in parts[1:]])\n",
    "\n",
    "                try:\n",
    "                    transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "                    if transformed['bboxes']:\n",
    "                        new_name = f\"{base_name}_aug_{class_id}_{i+1}\"\n",
    "                        new_img_path = os.path.join(IMG_DIR, new_name + os.path.splitext(img_path)[1])\n",
    "                        new_lbl_path = os.path.join(LBL_DIR, new_name + \".txt\")\n",
    "                        # transformed 딕셔너리에서 'image' 키로 값을 가져와야 합니다.\n",
    "                        cv2.imwrite(new_img_path, cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR))\n",
    "                        with open(new_lbl_path, 'w') as f:\n",
    "                            for j, bbox in enumerate(transformed['bboxes']):\n",
    "                                f.write(f\"{transformed['class_labels'][j]} {' '.join(map(str, bbox))}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  - 오류: {base_name} 증강 중 문제 발생 ({e})\")\n",
    "\n",
    "    print(\"\\n🎉 모든 소수 클래스 증강 작업이 완료되었습니다!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    augment_minority_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6f213856f8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 설정 ---\n",
    "BASE = r\"./object_detection_bok\"\n",
    "TARGET_IMAGE_COUNT = 100\n",
    "# --- 설정 ---\n",
    "\n",
    "IMG_DIR = os.path.join(BASE, \"images\", \"train\")\n",
    "LBL_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "def undersample_majority_classes():\n",
    "    \"\"\"다수 클래스를 분석하고 목표치에 도달할 때까지 관련 파일을 삭제합니다.\"\"\"\n",
    "    print(\"⚠️ 경고: 파일이 영구적으로 삭제됩니다. 5초 후에 작업을 시작합니다...\")\n",
    "    # import time; time.sleep(5) # 필요시 주석 해제\n",
    "\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"❌ 오류: 라벨 폴더 '{LBL_DIR}'를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    class_to_images = defaultdict(set)\n",
    "    for label_file in os.listdir(LBL_DIR):\n",
    "        if not label_file.endswith(\".txt\"): continue\n",
    "        image_name = os.path.splitext(label_file)[0]\n",
    "        with open(os.path.join(LBL_DIR, label_file), 'r') as f:\n",
    "            # [수정된 부분] float을 거쳐 int로 변환하도록 수정\n",
    "            classes_in_image = set(int(float(line.split()[0])) for line in f if line.strip())\n",
    "            for class_id in classes_in_image:\n",
    "                class_to_images[class_id].add(image_name)\n",
    "\n",
    "    class_image_counts = {k: len(v) for k, v in class_to_images.items()}\n",
    "    files_to_delete = set()\n",
    "\n",
    "    for class_id, image_count in class_image_counts.items():\n",
    "        if image_count > TARGET_IMAGE_COUNT:\n",
    "            num_to_delete = image_count - TARGET_IMAGE_COUNT\n",
    "            source_images = class_to_images[class_id]\n",
    "            images_marked_for_deletion = random.sample(list(source_images), num_to_delete)\n",
    "            files_to_delete.update(images_marked_for_deletion)\n",
    "            print(f\"🗑️ 클래스 {class_id}: {num_to_delete}개 이미지 삭제 예정...\")\n",
    "\n",
    "    print(f\"\\n총 {len(files_to_delete)}개의 이미지/라벨 쌍을 삭제합니다.\")\n",
    "    for base_name in files_to_delete:\n",
    "        lbl_path = os.path.join(LBL_DIR, base_name + \".txt\")\n",
    "        if os.path.exists(lbl_path): os.remove(lbl_path)\n",
    "\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            img_path = os.path.join(IMG_DIR, base_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                os.remove(img_path)\n",
    "                break\n",
    "\n",
    "    print(\"\\n🎉 모든 다수 클래스 언더샘플링 작업이 완료되었습니다!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    undersample_majority_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4f979ba1dfb122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:53:22.775683Z",
     "start_time": "2025-08-25T14:53:22.382088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 경고: 파일이 영구적으로 삭제됩니다. 5초 후에 작업을 시작합니다...\n",
      "🗑️ 클래스 1: 266개 이미지 삭제 예정...\n",
      "🗑️ 클래스 0: 11개 이미지 삭제 예정...\n",
      "\n",
      "총 273개의 이미지/라벨 쌍을 삭제합니다.\n",
      "\n",
      "🎉 모든 다수 클래스 언더샘플링 작업이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 설정 ---\n",
    "BASE = r\"./object_detection_\"\n",
    "TARGET_IMAGE_COUNT = 100\n",
    "# --- 설정 ---\n",
    "\n",
    "IMG_DIR = os.path.join(BASE, \"images\", \"train\")\n",
    "LBL_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "def undersample_majority_classes():\n",
    "    \"\"\"다수 클래스를 분석하고 목표치에 도달할 때까지 관련 파일을 삭제합니다.\"\"\"\n",
    "    print(\"⚠️ 경고: 파일이 영구적으로 삭제됩니다. 5초 후에 작업을 시작합니다...\")\n",
    "    # import time; time.sleep(5) # 필요시 주석 해제\n",
    "\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"❌ 오류: 라벨 폴더 '{LBL_DIR}'를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    class_to_images = defaultdict(set)\n",
    "    for label_file in os.listdir(LBL_DIR):\n",
    "        if not label_file.endswith(\".txt\"): continue\n",
    "        image_name = os.path.splitext(label_file)[0]\n",
    "        with open(os.path.join(LBL_DIR, label_file), 'r') as f:\n",
    "            # [수정된 부분] float을 거쳐 int로 변환하도록 수정\n",
    "            classes_in_image = set(int(float(line.split()[0])) for line in f if line.strip())\n",
    "            for class_id in classes_in_image:\n",
    "                class_to_images[class_id].add(image_name)\n",
    "\n",
    "    class_image_counts = {k: len(v) for k, v in class_to_images.items()}\n",
    "    files_to_delete = set()\n",
    "\n",
    "    for class_id, image_count in class_image_counts.items():\n",
    "        if image_count > TARGET_IMAGE_COUNT:\n",
    "            num_to_delete = image_count - TARGET_IMAGE_COUNT\n",
    "            source_images = class_to_images[class_id]\n",
    "            images_marked_for_deletion = random.sample(list(source_images), num_to_delete)\n",
    "            files_to_delete.update(images_marked_for_deletion)\n",
    "            print(f\"🗑️ 클래스 {class_id}: {num_to_delete}개 이미지 삭제 예정...\")\n",
    "\n",
    "    print(f\"\\n총 {len(files_to_delete)}개의 이미지/라벨 쌍을 삭제합니다.\")\n",
    "    for base_name in files_to_delete:\n",
    "        lbl_path = os.path.join(LBL_DIR, base_name + \".txt\")\n",
    "        if os.path.exists(lbl_path): os.remove(lbl_path)\n",
    "\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            img_path = os.path.join(IMG_DIR, base_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                os.remove(img_path)\n",
    "                break\n",
    "\n",
    "    print(\"\\n🎉 모든 다수 클래스 언더샘플링 작업이 완료되었습니다!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    undersample_majority_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8594ec26c4332da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:55:04.882677Z",
     "start_time": "2025-08-25T14:55:04.771792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 대상 폴더: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train\n",
      "\n",
      "📊 최종 클래스별 객체 수 (내림차순)\n",
      "------------------------------\n",
      "   Class  Count\n",
      "0      1    255\n",
      "1      5    100\n",
      "2      6    100\n",
      "3      2     98\n",
      "4      3     97\n",
      "5      4     96\n",
      "6      0     92\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- 설정 ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- 설정 ---\n",
    "\n",
    "LBL_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "def final_check_balance():\n",
    "    \"\"\"최종 클래스 분포(객체 수 기준)를 확인합니다.\"\"\"\n",
    "    print(f\"분석 대상 폴더: {os.path.abspath(LBL_DIR)}\")\n",
    "\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"❌ 오류: 라벨 폴더 '{LBL_DIR}'를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    class_ids = []\n",
    "    try:\n",
    "        for filename in os.listdir(LBL_DIR):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(LBL_DIR, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if parts:\n",
    "                            # '2.0' 같은 실수 형태의 클래스 ID도 처리 가능하도록 수정\n",
    "                            class_ids.append(int(float(parts[0])))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류: 파일을 읽는 중 문제가 발생했습니다: {e}\")\n",
    "        return\n",
    "\n",
    "    if not class_ids:\n",
    "        print(\"분석할 라벨 데이터가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    class_counts = Counter(class_ids)\n",
    "    df = pd.DataFrame(class_counts.items(), columns=['Class', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n📊 최종 클래스별 객체 수 (내림차순)\")\n",
    "    print(\"-\" * 30)\n",
    "    print(df_sorted.to_string())\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_check_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2640fddf5a8adb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:59:45.702217Z",
     "start_time": "2025-08-25T14:59:42.344374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "Installing collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4296aac191a222cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:05:23.210794Z",
     "start_time": "2025-08-25T15:05:21.396513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'val' 세트로 파일 이동 중... (69개)\n",
      "\n",
      "'test' 세트로 파일 이동 중... (134개)\n",
      "\n",
      "🎉 Iterative Stratification을 사용하여 파일 분할이 완료되었습니다!\n",
      "------------------------------\n",
      "  - 훈련(train) 세트: 471개\n",
      "  - 검증(val) 세트: 69개\n",
      "  - 테스트(test) 세트: 134개\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "# --- 설정 ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- 설정 ---\n",
    "\n",
    "IMG_SRC_DIR = os.path.join(BASE, \"images\", \"train\")\n",
    "LBL_SRC_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "# 1. 이미지와 클래스 라벨 매핑\n",
    "image_to_classes = defaultdict(set)\n",
    "all_labels = set()\n",
    "image_files = [os.path.splitext(f)[0] for f in os.listdir(LBL_SRC_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "for img_name in image_files:\n",
    "    with open(os.path.join(LBL_SRC_DIR, img_name + \".txt\"), 'r') as f:\n",
    "        classes_in_image = set(int(float(line.split()[0])) for line in f if line.strip())\n",
    "        image_to_classes[img_name] = classes_in_image\n",
    "        all_labels.update(classes_in_image)\n",
    "\n",
    "# 2. Iterative Stratification을 위한 데이터 형식 준비\n",
    "sorted_labels = sorted(list(all_labels))\n",
    "label_map = {label: i for i, label in enumerate(sorted_labels)}\n",
    "X = np.array(image_files).reshape(-1, 1)\n",
    "y = np.zeros((len(image_files), len(all_labels)), dtype=int)\n",
    "\n",
    "for i, img_name in enumerate(image_files):\n",
    "    for cls in image_to_classes[img_name]:\n",
    "        y[i, label_map[cls]] = 1\n",
    "\n",
    "# 3. 데이터 분할 (train: 70%, val: 20%, test: 10%)\n",
    "stratifier = IterativeStratification(n_splits=2, order=1, sample_distribution_per_fold=[0.3, 0.7])\n",
    "train_indices, temp_indices = next(stratifier.split(X, y))\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_temp, y_temp = X[temp_indices], y[temp_indices]\n",
    "\n",
    "stratifier_val_test = IterativeStratification(n_splits=2, order=1, sample_distribution_per_fold=[1/3, 2/3])\n",
    "test_indices, val_indices = next(stratifier_val_test.split(X_temp, y_temp)) # 1/3을 test로, 2/3를 val로\n",
    "X_val, y_val = X_temp[val_indices], y_temp[val_indices]\n",
    "X_test, y_test = X_temp[test_indices], y_temp[test_indices]\n",
    "\n",
    "train_files = set(X_train.flatten())\n",
    "val_files = set(X_val.flatten())\n",
    "test_files = set(X_test.flatten())\n",
    "\n",
    "# 4. 폴더 생성 및 파일 이동\n",
    "for split_name, file_set in [(\"val\", val_files), (\"test\", test_files)]:\n",
    "    os.makedirs(os.path.join(BASE, \"images\", split_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE, \"labels\", split_name), exist_ok=True)\n",
    "\n",
    "    print(f\"\\n'{split_name}' 세트로 파일 이동 중... ({len(file_set)}개)\")\n",
    "    for base_name in file_set:\n",
    "        # --- 여기에 파일 이동 로직 추가 ---\n",
    "        lbl_src_path = os.path.join(LBL_SRC_DIR, base_name + \".txt\")\n",
    "        lbl_dest_path = os.path.join(BASE, \"labels\", split_name, base_name + \".txt\")\n",
    "        if os.path.exists(lbl_src_path):\n",
    "            shutil.move(lbl_src_path, lbl_dest_path)\n",
    "\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            img_src_path = os.path.join(IMG_SRC_DIR, base_name + ext)\n",
    "            if os.path.exists(img_src_path):\n",
    "                img_dest_path = os.path.join(BASE, \"images\", split_name, base_name + ext)\n",
    "                shutil.move(img_src_path, img_dest_path)\n",
    "                break\n",
    "        # --- 로직 추가 끝 ---\n",
    "\n",
    "print(\"\\n🎉 Iterative Stratification을 사용하여 파일 분할이 완료되었습니다!\")\n",
    "print(\"-\" * 30)\n",
    "# train 폴더에 남은 파일 수를 세도록 수정\n",
    "print(f\"  - 훈련(train) 세트: {len(os.listdir(IMG_SRC_DIR))}개\")\n",
    "print(f\"  - 검증(val) 세트: {len(val_files)}개\")\n",
    "print(f\"  - 테스트(test) 세트: {len(test_files)}개\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2005a34a0bbcc7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:05:40.275193Z",
     "start_time": "2025-08-25T15:05:40.125912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 세트의 클래스 분포를 확인합니다...\n",
      "\n",
      "--- [TRAIN 세트] ---\n",
      "   Class  Count\n",
      "0      0     65\n",
      "1      1    192\n",
      "2      2     69\n",
      "3      3     68\n",
      "4      4     67\n",
      "5      5     70\n",
      "6      6     70\n",
      "\n",
      "--- [VAL 세트] ---\n",
      "   Class  Count\n",
      "0      0      9\n",
      "1      1     19\n",
      "2      2     10\n",
      "3      3     10\n",
      "4      4     10\n",
      "5      5     10\n",
      "6      6     10\n",
      "\n",
      "--- [TEST 세트] ---\n",
      "   Class  Count\n",
      "0      0     18\n",
      "1      1     44\n",
      "2      2     19\n",
      "3      3     19\n",
      "4      4     19\n",
      "5      5     20\n",
      "6      6     20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- 설정 ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- 설정 ---\n",
    "\n",
    "SPLITS_TO_CHECK = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "def analyze_split_distribution(split_name):\n",
    "    \"\"\"지정된 세트(train, val, test)의 클래스 분포를 분석합니다.\"\"\"\n",
    "\n",
    "    lbl_dir = os.path.join(BASE, \"labels\", split_name)\n",
    "\n",
    "    if not os.path.isdir(lbl_dir):\n",
    "        print(f\"\\n--- [{split_name.upper()} 세트] ---\")\n",
    "        print(f\"❌ 오류: '{lbl_dir}' 폴더를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    class_ids = []\n",
    "    for filename in os.listdir(lbl_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(lbl_dir, filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if parts:\n",
    "                        class_ids.append(int(float(parts[0])))\n",
    "\n",
    "    print(f\"\\n--- [{split_name.upper()} 세트] ---\")\n",
    "    if not class_ids:\n",
    "        print(\"분석할 라벨 데이터가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    class_counts = Counter(class_ids)\n",
    "    df = pd.DataFrame(class_counts.items(), columns=['Class', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Class').reset_index(drop=True) # 클래스 ID 순으로 정렬\n",
    "\n",
    "    print(df_sorted.to_string())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"각 세트의 클래스 분포를 확인합니다...\")\n",
    "    for split in SPLITS_TO_CHECK:\n",
    "        analyze_split_distribution(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f31511893579c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-26T00:45:34.285276Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data.yaml 이미 존재: ./object_detection_\\data.yaml\n",
      "[OK] 라벨 매칭 이상 없음\n",
      "[INFO] Loading model: ./yolov8n.pt\n",
      "[INFO] Start training...\n",
      "New https://pypi.org/project/ultralytics/8.3.186 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.176  Python-3.11.2 torch-2.7.1+cpu CPU (AMD Ryzen 5 5600U with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./object_detection_\\data.yaml, degrees=5, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.15, mode=train, model=./yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_car_yolov8n, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=./object_detection_/runs_yolo_bok, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=object_detection_\\runs_yolo_bok\\exp_car_yolov8n, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.1, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.05, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 433.2197.8 MB/s, size: 217.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train... 471 images, 0 backgrounds, 0 corrupt: 100%|██████████| 471/471 [00:00<00:00, 814.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 791.6264.0 MB/s, size: 217.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\val... 69 images, 0 backgrounds, 0 corrupt: 100%|██████████| 69/69 [00:00<00:00, 1024.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to object_detection_\\runs_yolo_bok\\exp_car_yolov8n\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mobject_detection_\\runs_yolo_bok\\exp_car_yolov8n\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100         0G      1.489      4.698      1.258         43        640:  57%|█████▋    | 17/30 [01:35<01:13,  5.67s/it]"
     ]
    }
   ],
   "source": [
    "# train_yolo.py\n",
    "# -----------------------------\n",
    "# Ultralytics YOLO 학습 및 PTQ 적용 스크립트\n",
    "# -----------------------------\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ========= 사용자 설정 =========\n",
    "DATASET_DIR = r\"./object_detection_\"\n",
    "PROJECT_DIR = r\"./object_detection_/runs_yolo_bok\"\n",
    "EXP_NAME    = \"exp_car_yolov8n\"\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"animal\", \"person\", \"traffic_red\",\"traffic_yellow\", \"traffic_green\", \"right\", \"left\"\n",
    "]\n",
    "\n",
    "MODEL_NAME = \"./yolov8n.pt\"\n",
    "EPOCHS     = 100\n",
    "IMGSZ      = 320\n",
    "BATCH      = 16\n",
    "LR0        = 0.005\n",
    "PATIENCE   = 10\n",
    "DEVICE     = \"cpu\"\n",
    "\n",
    "DO_PREDICT_SAMPLES = True\n",
    "DO_EXPORT_OPENVINO = True # PTQ를 적용할 것이므로 True로 유지\n",
    "DO_EXPORT_ONNX     = False\n",
    "\n",
    "PREDICT_SOURCE = os.path.join(DATASET_DIR, \"images\", \"val\")\n",
    "PREDICT_CONF   = 0.25\n",
    "\n",
    "# ========= 유틸 =========\n",
    "def ensure_yaml(dataset_dir, class_names):\n",
    "    \"\"\"data.yaml 자동 생성 (이미 있으면 덮어쓰지 않음)\"\"\"\n",
    "    yaml_path = os.path.join(dataset_dir, \"data.yaml\")\n",
    "    if os.path.exists(yaml_path):\n",
    "        print(f\"[INFO] data.yaml 이미 존재: {yaml_path}\")\n",
    "        return yaml_path\n",
    "\n",
    "    # 절대 경로로 변환하여 yaml 파일에 기록\n",
    "    abs_dataset_dir = os.path.abspath(dataset_dir)\n",
    "    content = [\n",
    "        f\"path: {abs_dataset_dir}\", # 절대 경로 사용\n",
    "        \"train: images/train\",\n",
    "        \"val: images/val\",\n",
    "        \"test: images/test\",\n",
    "        \"names:\"\n",
    "    ]\n",
    "    for i, name in enumerate(class_names):\n",
    "        content.append(f\"  {i}: {name}\")\n",
    "\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(content) + \"\\n\")\n",
    "\n",
    "    print(f\"[OK] data.yaml 생성: {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "def sanity_check(dataset_dir):\n",
    "    \"\"\"간단 무결성 체크\"\"\"\n",
    "    img_train_dir = os.path.join(dataset_dir, \"images\", \"train\")\n",
    "    lbl_train_dir = os.path.join(dataset_dir, \"labels\", \"train\")\n",
    "    if not os.path.isdir(img_train_dir) or not os.path.isdir(lbl_train_dir):\n",
    "        print(f\"[WARN] train 폴더를 찾을 수 없어 sanity check를 건너뜁니다.\")\n",
    "        return\n",
    "\n",
    "    missing = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "    for f in os.listdir(img_train_dir):\n",
    "        ext = os.path.splitext(f)[1].lower()\n",
    "        if ext not in exts: continue\n",
    "        stem = os.path.splitext(f)[0]\n",
    "        if not os.path.exists(os.path.join(lbl_train_dir, stem + \".txt\")):\n",
    "            missing.append(f)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"[WARN] 라벨 누락 이미지 {len(missing)}개 예시: {missing[:10]}\")\n",
    "    else:\n",
    "        print(\"[OK] 라벨 매칭 이상 없음\")\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 1) data.yaml 보장\n",
    "    data_yaml = ensure_yaml(DATASET_DIR, CLASS_NAMES)\n",
    "\n",
    "    # 2) 간단 체크\n",
    "    sanity_check(DATASET_DIR)\n",
    "\n",
    "    # 3) 모델 로드\n",
    "    print(f\"[INFO] Loading model: {MODEL_NAME}\")\n",
    "    model = YOLO(MODEL_NAME)\n",
    "\n",
    "    # 4) 학습\n",
    "    print(\"[INFO] Start training...\")\n",
    "    results = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMGSZ,\n",
    "        batch=BATCH,\n",
    "        device=DEVICE,\n",
    "        project=PROJECT_DIR,\n",
    "        name=EXP_NAME,\n",
    "        lr0=LR0,\n",
    "        patience=PATIENCE,\n",
    "        optimizer=\"auto\",\n",
    "        hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,\n",
    "        fliplr=0.5,\n",
    "        mosaic=1.0, mixup=0.15,\n",
    "        degrees=5, translate=0.05, scale=0.1, shear=0.0, perspective=0.0,\n",
    "    )\n",
    "\n",
    "    run_dir = results.save_dir\n",
    "    best_pt = os.path.join(run_dir, \"weights\", \"best.pt\")\n",
    "    print(f\"[OK] Training done. best: {best_pt}\")\n",
    "\n",
    "    # 5) 검증(mAP, PR커브)\n",
    "    print(\"[INFO] Validate best weights...\")\n",
    "    model = YOLO(best_pt)\n",
    "    model.val(data=data_yaml, project=PROJECT_DIR, name=f\"{EXP_NAME}_val\")\n",
    "\n",
    "    # 6) 예측 샘플 저장\n",
    "    if DO_PREDICT_SAMPLES and os.path.exists(PREDICT_SOURCE):\n",
    "        print(f\"[INFO] Predict & save samples from: {PREDICT_SOURCE}\")\n",
    "        model.predict(\n",
    "            source=PREDICT_SOURCE,\n",
    "            conf=PREDICT_CONF,\n",
    "            save=True,\n",
    "            project=PROJECT_DIR,\n",
    "            name=f\"{EXP_NAME}_pred_val\"\n",
    "        )\n",
    "\n",
    "     # 7) 내보내기 (PTQ 적용)\n",
    "    if DO_EXPORT_OPENVINO:\n",
    "        print(\"[INFO] Export OpenVINO IR with Data-aware INT8 Quantization (PTQ)...\")\n",
    "\n",
    "        # ✅ 'data'와 'imgsz'를 명시하여 우리 데이터셋에 맞게 양자화를 수행합니다.\n",
    "        model.export(\n",
    "            format=\"openvino\",    # OpenVINO 형식으로 내보내기\n",
    "            int8=True,            # INT8 양자화 활성화\n",
    "            data=data_yaml,       # 교정 데이터로 우리 val 세트를 사용하도록 지정\n",
    "            imgsz=IMGSZ,          # 훈련 시와 동일한 이미지 크기로 교정\n",
    "            half=False,           # FP16 대신 INT8을 목표로 하므로 False\n",
    "            simplify=True         # ONNX 모델을 단순화하여 호환성 및 속도 향상\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"[DONE] All finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97dcfc1c6e02415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861836d339b609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd48da4823fd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b705bffa7e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74d82d81766055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804b5f730351b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
