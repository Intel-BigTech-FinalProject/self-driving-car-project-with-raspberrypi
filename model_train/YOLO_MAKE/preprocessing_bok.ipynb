{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:03:33.963596Z",
     "start_time": "2025-08-25T14:03:33.954126Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wjbok\\\\Desktop\\\\BigTech'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1048a7fbe7e0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:04:03.410793Z",
     "start_time": "2025-08-25T14:04:02.376248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶„ì„ ëŒ€ìƒ í´ë”: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train\n",
      "\n",
      "âœ… í´ë˜ìŠ¤ë³„ ê°ì²´ ìˆ˜ (ë‚´ë¦¼ì°¨ìˆœ)\n",
      "------------------------------\n",
      "   Class  Count\n",
      "0      1    958\n",
      "1      0    112\n",
      "2      5     59\n",
      "3      3     49\n",
      "4      4     36\n",
      "5      6     28\n",
      "6      2     22\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "BASE = r\"./object_detection_\"\n",
    "LBL_SRC = os.path.join(BASE,\"labels\",\"train\")\n",
    "\n",
    "\n",
    "def analyze_class_distribution(label_directory):\n",
    "    \"\"\"\n",
    "    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ë¼ë²¨ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ í‘œë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"ë¶„ì„ ëŒ€ìƒ í´ë”: {os.path.abspath(label_directory)}\")\n",
    "\n",
    "    # 1. ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    if not os.path.isdir(label_directory):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: '{label_directory}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    # 2. ëª¨ë“  í´ë˜ìŠ¤ ID ìˆ˜ì§‘\n",
    "    class_ids = []\n",
    "    try:\n",
    "        label_files = [f for f in os.listdir(label_directory) if f.endswith(\".txt\")]\n",
    "        if not label_files:\n",
    "            print(f\"âš ï¸ ê²½ê³ : '{label_directory}' í´ë”ì— ë¼ë²¨ íŒŒì¼(.txt)ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "\n",
    "        for filename in label_files:\n",
    "            filepath = os.path.join(label_directory, filename)\n",
    "            with open(filepath, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    if parts:\n",
    "                        try:\n",
    "                            class_id = int(parts[0])\n",
    "                            class_ids.append(class_id)\n",
    "                        except (ValueError, IndexError):\n",
    "                            continue # ìˆ«ìê°€ ì•„ë‹ˆê±°ë‚˜ ë¹ˆ ì¤„ì¸ ê²½ìš° ë¬´ì‹œ\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return\n",
    "\n",
    "    if not class_ids:\n",
    "        print(\"ë¶„ì„í•  í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 3. í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ê³„ì‚° ë° í‘œë¡œ ì¶œë ¥\n",
    "    class_counts = Counter(class_ids)\n",
    "    df = pd.DataFrame(class_counts.items(), columns=['Class', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nâœ… í´ë˜ìŠ¤ë³„ ê°ì²´ ìˆ˜ (ë‚´ë¦¼ì°¨ìˆœ)\")\n",
    "    print(\"-\" * 30)\n",
    "    print(df_sorted.to_string())\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰\n",
    "if __name__ == '__main__':\n",
    "    analyze_class_distribution(LBL_SRC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85a583535fb4f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:05:33.004021Z",
     "start_time": "2025-08-25T14:05:02.170804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from albumentations) (1.16.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from albumentations) (6.0.1)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Downloading stringzilla-3.12.6-cp311-cp311-win_amd64.whl.metadata (81 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Downloading simsimd-6.5.1-cp311-cp311-win_amd64.whl.metadata (72 kB)\n",
      "Collecting numpy>=1.24.4 (from albumentations)\n",
      "  Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\wjbok\\desktop\\bigtech\\venv\\lib\\site-packages (from pydantic>=2.9.2->albumentations) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Downloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/38.9 MB 10.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.2/38.9 MB 11.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 6.8/38.9 MB 11.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 8.7/38.9 MB 10.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.0/38.9 MB 10.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.4/38.9 MB 10.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.7/38.9 MB 10.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 17.8/38.9 MB 10.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.2/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 22.3/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.6/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 27.3/38.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 29.6/38.9 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.7/38.9 MB 10.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.3/38.9 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.9 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.9/38.9 MB 10.5 MB/s  0:00:03\n",
      "Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 9.8 MB/s  0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading simsimd-6.5.1-cp311-cp311-win_amd64.whl (94 kB)\n",
      "Downloading stringzilla-3.12.6-cp311-cp311-win_amd64.whl (79 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: stringzilla, simsimd, typing-inspection, pydantic-core, numpy, annotated-types, pydantic, opencv-python-headless, albucore, albumentations\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 1.26.4\n",
      "\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ---------------- -----------------------  4/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ------------------------ ---------------  6/10 [pydantic]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   ---------------------------- -----------  7/10 [opencv-python-headless]\n",
      "   -------------------------------- -------  8/10 [albucore]\n",
      "   ------------------------------------ ---  9/10 [albumentations]\n",
      "   ------------------------------------ ---  9/10 [albumentations]\n",
      "   ---------------------------------------- 10/10 [albumentations]\n",
      "\n",
      "Successfully installed albucore-0.0.24 albumentations-2.0.8 annotated-types-0.7.0 numpy-2.2.6 opencv-python-headless-4.12.0.88 pydantic-2.11.7 pydantic-core-2.33.2 simsimd-6.5.1 stringzilla-3.12.6 typing-inspection-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datumaro 1.10.0 requires numpy<2,>=1.23.4, but you have numpy 2.2.6 which is incompatible.\n",
      "mediapipe 0.10.14 requires protobuf<5,>=4.25.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "tensorflow-intel 2.14.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cedc29d60c70b381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:26:33.540758Z",
     "start_time": "2025-08-25T14:26:30.178522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...\n",
      "\n",
      "âœ… ì „ì²´ ë°ì´í„°ì…‹ì˜ í•´ìƒë„ ë¶„í¬\n",
      "-----------------------------------\n",
      "  Resolution  Count\n",
      "0    640x360    650\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "SPLITS_TO_CHECK = [\"train\"]\n",
    "all_resolutions = []\n",
    "\n",
    "print(\"ì´ë¯¸ì§€ í•´ìƒë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...\")\n",
    "for split in SPLITS_TO_CHECK:\n",
    "    img_dir = os.path.join(BASE, \"images\", split)\n",
    "    if not os.path.isdir(img_dir):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                img_path = os.path.join(img_dir, filename)\n",
    "                # OpenCVë¡œ ì´ë¯¸ì§€ í¬ê¸°(ë„ˆë¹„, ë†’ì´) ì½ê¸°\n",
    "                image = cv2.imread(img_path)\n",
    "                height, width, _ = image.shape\n",
    "                all_resolutions.append(f\"{width}x{height}\")\n",
    "            except Exception as e:\n",
    "                print(f\"'{filename}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "if not all_resolutions:\n",
    "    print(\"ë¶„ì„í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    resolution_counts = Counter(all_resolutions)\n",
    "    df = pd.DataFrame(resolution_counts.items(), columns=['Resolution', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nâœ… ì „ì²´ ë°ì´í„°ì…‹ì˜ í•´ìƒë„ ë¶„í¬\")\n",
    "    print(\"-\" * 35)\n",
    "    print(df_sorted.to_string())\n",
    "    print(\"-\" * 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae6e04e07aae66b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:47:39.888648Z",
     "start_time": "2025-08-25T14:47:34.289999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\wjbok\\AppData\\Local\\Temp\\ipykernel_664\\205868126.py:13: UserWarning: Argument(s) 'value' are not valid for transform ShiftScaleRotate\n",
      "  A.ShiftScaleRotate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ì¦ê°• ì „ í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ìˆ˜ ---\n",
      "  - í´ë˜ìŠ¤ 0: 111ê°œ ì´ë¯¸ì§€\n",
      "  - í´ë˜ìŠ¤ 1: 366ê°œ ì´ë¯¸ì§€\n",
      "  - í´ë˜ìŠ¤ 2: 22ê°œ ì´ë¯¸ì§€\n",
      "  - í´ë˜ìŠ¤ 3: 49ê°œ ì´ë¯¸ì§€\n",
      "  - í´ë˜ìŠ¤ 4: 36ê°œ ì´ë¯¸ì§€\n",
      "  - í´ë˜ìŠ¤ 5: 59ê°œ ì´ë¯¸ì§€\n",
      "  - í´ë˜ìŠ¤ 6: 28ê°œ ì´ë¯¸ì§€\n",
      "-----------------------------------\n",
      "ğŸ”§ í´ë˜ìŠ¤ 3 ì¦ê°• ì‹œì‘ (ëª©í‘œ: 100ê°œ, ìƒì„±: 51ê°œ)\n",
      "ğŸ”§ í´ë˜ìŠ¤ 5 ì¦ê°• ì‹œì‘ (ëª©í‘œ: 100ê°œ, ìƒì„±: 41ê°œ)\n",
      "ğŸ”§ í´ë˜ìŠ¤ 6 ì¦ê°• ì‹œì‘ (ëª©í‘œ: 100ê°œ, ìƒì„±: 72ê°œ)\n",
      "ğŸ”§ í´ë˜ìŠ¤ 2 ì¦ê°• ì‹œì‘ (ëª©í‘œ: 100ê°œ, ìƒì„±: 78ê°œ)\n",
      "ğŸ”§ í´ë˜ìŠ¤ 4 ì¦ê°• ì‹œì‘ (ëª©í‘œ: 100ê°œ, ìƒì„±: 64ê°œ)\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ì†Œìˆ˜ í´ë˜ìŠ¤ ì¦ê°• ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "TARGET_IMAGE_COUNT=100\n",
    "\n",
    "IMG_DIR = os.path.join(BASE,\"images\",\"train\")\n",
    "LBL_DIR = os.path.join(BASE,\"labels\",\"train\")\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.0625,\n",
    "        scale_limit=0.1,\n",
    "        rotate_limit=15,\n",
    "        p=0.5,\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        value=0\n",
    "    ),\n",
    "    A.RandomResizedCrop(\n",
    "        size=(352, 640),\n",
    "        scale=(0.8, 1.0),\n",
    "        p=0.3\n",
    "    ),\n",
    "], bbox_params=A.BboxParams(\n",
    "        format='yolo',\n",
    "        label_fields=['class_labels'],\n",
    "        min_visibility=0.3\n",
    "))\n",
    "\n",
    "def augment_minority_classes():\n",
    "    \"\"\"ì†Œìˆ˜ í´ë˜ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ëª©í‘œì¹˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì¦ê°•í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ë¼ë²¨ í´ë” '{LBL_DIR}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    class_to_images = defaultdict(list)\n",
    "    for label_file in os.listdir(LBL_DIR):\n",
    "        if not label_file.endswith(\".txt\"): continue\n",
    "        image_name = os.path.splitext(label_file)[0]\n",
    "        with open(os.path.join(LBL_DIR, label_file), 'r') as f:\n",
    "            classes_in_image = set(int(line.split()[0]) for line in f if line.strip())\n",
    "            for class_id in classes_in_image:\n",
    "                class_to_images[class_id].append(image_name)\n",
    "\n",
    "    class_image_counts = {k: len(v) for k, v in class_to_images.items()}\n",
    "    print(\"--- ì¦ê°• ì „ í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ìˆ˜ ---\")\n",
    "    for cid, count in sorted(class_image_counts.items()):\n",
    "        print(f\"  - í´ë˜ìŠ¤ {cid}: {count}ê°œ ì´ë¯¸ì§€\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for class_id, image_count in class_image_counts.items():\n",
    "        if image_count < TARGET_IMAGE_COUNT:\n",
    "            num_to_generate = TARGET_IMAGE_COUNT - image_count\n",
    "            source_images = class_to_images[class_id]\n",
    "            print(f\"ğŸ”§ í´ë˜ìŠ¤ {class_id} ì¦ê°• ì‹œì‘ (ëª©í‘œ: {TARGET_IMAGE_COUNT}ê°œ, ìƒì„±: {num_to_generate}ê°œ)\")\n",
    "\n",
    "            for i in range(num_to_generate):\n",
    "                base_name = random.choice(source_images)\n",
    "                img_path, lbl_path = None, os.path.join(LBL_DIR, base_name + \".txt\")\n",
    "                for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                    if os.path.exists(os.path.join(IMG_DIR, base_name + ext)):\n",
    "                        img_path = os.path.join(IMG_DIR, base_name + ext)\n",
    "                        break\n",
    "                if not img_path: continue\n",
    "\n",
    "                image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "                bboxes, class_labels = [], []\n",
    "                with open(lbl_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        class_labels.append(int(parts[0]))\n",
    "                        bboxes.append([float(p) for p in parts[1:]])\n",
    "\n",
    "                try:\n",
    "                    transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "                    if transformed['bboxes']:\n",
    "                        new_name = f\"{base_name}_aug_{class_id}_{i+1}\"\n",
    "                        new_img_path = os.path.join(IMG_DIR, new_name + os.path.splitext(img_path)[1])\n",
    "                        new_lbl_path = os.path.join(LBL_DIR, new_name + \".txt\")\n",
    "                        # transformed ë”•ì…”ë„ˆë¦¬ì—ì„œ 'image' í‚¤ë¡œ ê°’ì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
    "                        cv2.imwrite(new_img_path, cv2.cvtColor(transformed['image'], cv2.COLOR_RGB2BGR))\n",
    "                        with open(new_lbl_path, 'w') as f:\n",
    "                            for j, bbox in enumerate(transformed['bboxes']):\n",
    "                                f.write(f\"{transformed['class_labels'][j]} {' '.join(map(str, bbox))}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  - ì˜¤ë¥˜: {base_name} ì¦ê°• ì¤‘ ë¬¸ì œ ë°œìƒ ({e})\")\n",
    "\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ì†Œìˆ˜ í´ë˜ìŠ¤ ì¦ê°• ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    augment_minority_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6f213856f8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "BASE = r\"./object_detection_bok\"\n",
    "TARGET_IMAGE_COUNT = 100\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "IMG_DIR = os.path.join(BASE, \"images\", \"train\")\n",
    "LBL_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "def undersample_majority_classes():\n",
    "    \"\"\"ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ëª©í‘œì¹˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ê´€ë ¨ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"âš ï¸ ê²½ê³ : íŒŒì¼ì´ ì˜êµ¬ì ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤. 5ì´ˆ í›„ì— ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    # import time; time.sleep(5) # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ\n",
    "\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ë¼ë²¨ í´ë” '{LBL_DIR}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    class_to_images = defaultdict(set)\n",
    "    for label_file in os.listdir(LBL_DIR):\n",
    "        if not label_file.endswith(\".txt\"): continue\n",
    "        image_name = os.path.splitext(label_file)[0]\n",
    "        with open(os.path.join(LBL_DIR, label_file), 'r') as f:\n",
    "            # [ìˆ˜ì •ëœ ë¶€ë¶„] floatì„ ê±°ì³ intë¡œ ë³€í™˜í•˜ë„ë¡ ìˆ˜ì •\n",
    "            classes_in_image = set(int(float(line.split()[0])) for line in f if line.strip())\n",
    "            for class_id in classes_in_image:\n",
    "                class_to_images[class_id].add(image_name)\n",
    "\n",
    "    class_image_counts = {k: len(v) for k, v in class_to_images.items()}\n",
    "    files_to_delete = set()\n",
    "\n",
    "    for class_id, image_count in class_image_counts.items():\n",
    "        if image_count > TARGET_IMAGE_COUNT:\n",
    "            num_to_delete = image_count - TARGET_IMAGE_COUNT\n",
    "            source_images = class_to_images[class_id]\n",
    "            images_marked_for_deletion = random.sample(list(source_images), num_to_delete)\n",
    "            files_to_delete.update(images_marked_for_deletion)\n",
    "            print(f\"ğŸ—‘ï¸ í´ë˜ìŠ¤ {class_id}: {num_to_delete}ê°œ ì´ë¯¸ì§€ ì‚­ì œ ì˜ˆì •...\")\n",
    "\n",
    "    print(f\"\\nì´ {len(files_to_delete)}ê°œì˜ ì´ë¯¸ì§€/ë¼ë²¨ ìŒì„ ì‚­ì œí•©ë‹ˆë‹¤.\")\n",
    "    for base_name in files_to_delete:\n",
    "        lbl_path = os.path.join(LBL_DIR, base_name + \".txt\")\n",
    "        if os.path.exists(lbl_path): os.remove(lbl_path)\n",
    "\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            img_path = os.path.join(IMG_DIR, base_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                os.remove(img_path)\n",
    "                break\n",
    "\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ë‹¤ìˆ˜ í´ë˜ìŠ¤ ì–¸ë”ìƒ˜í”Œë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    undersample_majority_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4f979ba1dfb122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:53:22.775683Z",
     "start_time": "2025-08-25T14:53:22.382088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ê²½ê³ : íŒŒì¼ì´ ì˜êµ¬ì ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤. 5ì´ˆ í›„ì— ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n",
      "ğŸ—‘ï¸ í´ë˜ìŠ¤ 1: 266ê°œ ì´ë¯¸ì§€ ì‚­ì œ ì˜ˆì •...\n",
      "ğŸ—‘ï¸ í´ë˜ìŠ¤ 0: 11ê°œ ì´ë¯¸ì§€ ì‚­ì œ ì˜ˆì •...\n",
      "\n",
      "ì´ 273ê°œì˜ ì´ë¯¸ì§€/ë¼ë²¨ ìŒì„ ì‚­ì œí•©ë‹ˆë‹¤.\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ë‹¤ìˆ˜ í´ë˜ìŠ¤ ì–¸ë”ìƒ˜í”Œë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "BASE = r\"./object_detection_\"\n",
    "TARGET_IMAGE_COUNT = 100\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "IMG_DIR = os.path.join(BASE, \"images\", \"train\")\n",
    "LBL_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "def undersample_majority_classes():\n",
    "    \"\"\"ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ëª©í‘œì¹˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ê´€ë ¨ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"âš ï¸ ê²½ê³ : íŒŒì¼ì´ ì˜êµ¬ì ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤. 5ì´ˆ í›„ì— ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    # import time; time.sleep(5) # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ\n",
    "\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ë¼ë²¨ í´ë” '{LBL_DIR}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    class_to_images = defaultdict(set)\n",
    "    for label_file in os.listdir(LBL_DIR):\n",
    "        if not label_file.endswith(\".txt\"): continue\n",
    "        image_name = os.path.splitext(label_file)[0]\n",
    "        with open(os.path.join(LBL_DIR, label_file), 'r') as f:\n",
    "            # [ìˆ˜ì •ëœ ë¶€ë¶„] floatì„ ê±°ì³ intë¡œ ë³€í™˜í•˜ë„ë¡ ìˆ˜ì •\n",
    "            classes_in_image = set(int(float(line.split()[0])) for line in f if line.strip())\n",
    "            for class_id in classes_in_image:\n",
    "                class_to_images[class_id].add(image_name)\n",
    "\n",
    "    class_image_counts = {k: len(v) for k, v in class_to_images.items()}\n",
    "    files_to_delete = set()\n",
    "\n",
    "    for class_id, image_count in class_image_counts.items():\n",
    "        if image_count > TARGET_IMAGE_COUNT:\n",
    "            num_to_delete = image_count - TARGET_IMAGE_COUNT\n",
    "            source_images = class_to_images[class_id]\n",
    "            images_marked_for_deletion = random.sample(list(source_images), num_to_delete)\n",
    "            files_to_delete.update(images_marked_for_deletion)\n",
    "            print(f\"ğŸ—‘ï¸ í´ë˜ìŠ¤ {class_id}: {num_to_delete}ê°œ ì´ë¯¸ì§€ ì‚­ì œ ì˜ˆì •...\")\n",
    "\n",
    "    print(f\"\\nì´ {len(files_to_delete)}ê°œì˜ ì´ë¯¸ì§€/ë¼ë²¨ ìŒì„ ì‚­ì œí•©ë‹ˆë‹¤.\")\n",
    "    for base_name in files_to_delete:\n",
    "        lbl_path = os.path.join(LBL_DIR, base_name + \".txt\")\n",
    "        if os.path.exists(lbl_path): os.remove(lbl_path)\n",
    "\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            img_path = os.path.join(IMG_DIR, base_name + ext)\n",
    "            if os.path.exists(img_path):\n",
    "                os.remove(img_path)\n",
    "                break\n",
    "\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ë‹¤ìˆ˜ í´ë˜ìŠ¤ ì–¸ë”ìƒ˜í”Œë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    undersample_majority_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8594ec26c4332da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:55:04.882677Z",
     "start_time": "2025-08-25T14:55:04.771792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶„ì„ ëŒ€ìƒ í´ë”: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train\n",
      "\n",
      "ğŸ“Š ìµœì¢… í´ë˜ìŠ¤ë³„ ê°ì²´ ìˆ˜ (ë‚´ë¦¼ì°¨ìˆœ)\n",
      "------------------------------\n",
      "   Class  Count\n",
      "0      1    255\n",
      "1      5    100\n",
      "2      6    100\n",
      "3      2     98\n",
      "4      3     97\n",
      "5      4     96\n",
      "6      0     92\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "LBL_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "def final_check_balance():\n",
    "    \"\"\"ìµœì¢… í´ë˜ìŠ¤ ë¶„í¬(ê°ì²´ ìˆ˜ ê¸°ì¤€)ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(f\"ë¶„ì„ ëŒ€ìƒ í´ë”: {os.path.abspath(LBL_DIR)}\")\n",
    "\n",
    "    if not os.path.isdir(LBL_DIR):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: ë¼ë²¨ í´ë” '{LBL_DIR}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    class_ids = []\n",
    "    try:\n",
    "        for filename in os.listdir(LBL_DIR):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                with open(os.path.join(LBL_DIR, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if parts:\n",
    "                            # '2.0' ê°™ì€ ì‹¤ìˆ˜ í˜•íƒœì˜ í´ë˜ìŠ¤ IDë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ìˆ˜ì •\n",
    "                            class_ids.append(int(float(parts[0])))\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return\n",
    "\n",
    "    if not class_ids:\n",
    "        print(\"ë¶„ì„í•  ë¼ë²¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    class_counts = Counter(class_ids)\n",
    "    df = pd.DataFrame(class_counts.items(), columns=['Class', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nğŸ“Š ìµœì¢… í´ë˜ìŠ¤ë³„ ê°ì²´ ìˆ˜ (ë‚´ë¦¼ì°¨ìˆœ)\")\n",
    "    print(\"-\" * 30)\n",
    "    print(df_sorted.to_string())\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_check_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2640fddf5a8adb9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T14:59:45.702217Z",
     "start_time": "2025-08-25T14:59:42.344374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "Installing collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4296aac191a222cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:05:23.210794Z",
     "start_time": "2025-08-25T15:05:21.396513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'val' ì„¸íŠ¸ë¡œ íŒŒì¼ ì´ë™ ì¤‘... (69ê°œ)\n",
      "\n",
      "'test' ì„¸íŠ¸ë¡œ íŒŒì¼ ì´ë™ ì¤‘... (134ê°œ)\n",
      "\n",
      "ğŸ‰ Iterative Stratificationì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
      "------------------------------\n",
      "  - í›ˆë ¨(train) ì„¸íŠ¸: 471ê°œ\n",
      "  - ê²€ì¦(val) ì„¸íŠ¸: 69ê°œ\n",
      "  - í…ŒìŠ¤íŠ¸(test) ì„¸íŠ¸: 134ê°œ\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "IMG_SRC_DIR = os.path.join(BASE, \"images\", \"train\")\n",
    "LBL_SRC_DIR = os.path.join(BASE, \"labels\", \"train\")\n",
    "\n",
    "# 1. ì´ë¯¸ì§€ì™€ í´ë˜ìŠ¤ ë¼ë²¨ ë§¤í•‘\n",
    "image_to_classes = defaultdict(set)\n",
    "all_labels = set()\n",
    "image_files = [os.path.splitext(f)[0] for f in os.listdir(LBL_SRC_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "for img_name in image_files:\n",
    "    with open(os.path.join(LBL_SRC_DIR, img_name + \".txt\"), 'r') as f:\n",
    "        classes_in_image = set(int(float(line.split()[0])) for line in f if line.strip())\n",
    "        image_to_classes[img_name] = classes_in_image\n",
    "        all_labels.update(classes_in_image)\n",
    "\n",
    "# 2. Iterative Stratificationì„ ìœ„í•œ ë°ì´í„° í˜•ì‹ ì¤€ë¹„\n",
    "sorted_labels = sorted(list(all_labels))\n",
    "label_map = {label: i for i, label in enumerate(sorted_labels)}\n",
    "X = np.array(image_files).reshape(-1, 1)\n",
    "y = np.zeros((len(image_files), len(all_labels)), dtype=int)\n",
    "\n",
    "for i, img_name in enumerate(image_files):\n",
    "    for cls in image_to_classes[img_name]:\n",
    "        y[i, label_map[cls]] = 1\n",
    "\n",
    "# 3. ë°ì´í„° ë¶„í•  (train: 70%, val: 20%, test: 10%)\n",
    "stratifier = IterativeStratification(n_splits=2, order=1, sample_distribution_per_fold=[0.3, 0.7])\n",
    "train_indices, temp_indices = next(stratifier.split(X, y))\n",
    "X_train, y_train = X[train_indices], y[train_indices]\n",
    "X_temp, y_temp = X[temp_indices], y[temp_indices]\n",
    "\n",
    "stratifier_val_test = IterativeStratification(n_splits=2, order=1, sample_distribution_per_fold=[1/3, 2/3])\n",
    "test_indices, val_indices = next(stratifier_val_test.split(X_temp, y_temp)) # 1/3ì„ testë¡œ, 2/3ë¥¼ valë¡œ\n",
    "X_val, y_val = X_temp[val_indices], y_temp[val_indices]\n",
    "X_test, y_test = X_temp[test_indices], y_temp[test_indices]\n",
    "\n",
    "train_files = set(X_train.flatten())\n",
    "val_files = set(X_val.flatten())\n",
    "test_files = set(X_test.flatten())\n",
    "\n",
    "# 4. í´ë” ìƒì„± ë° íŒŒì¼ ì´ë™\n",
    "for split_name, file_set in [(\"val\", val_files), (\"test\", test_files)]:\n",
    "    os.makedirs(os.path.join(BASE, \"images\", split_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(BASE, \"labels\", split_name), exist_ok=True)\n",
    "\n",
    "    print(f\"\\n'{split_name}' ì„¸íŠ¸ë¡œ íŒŒì¼ ì´ë™ ì¤‘... ({len(file_set)}ê°œ)\")\n",
    "    for base_name in file_set:\n",
    "        # --- ì—¬ê¸°ì— íŒŒì¼ ì´ë™ ë¡œì§ ì¶”ê°€ ---\n",
    "        lbl_src_path = os.path.join(LBL_SRC_DIR, base_name + \".txt\")\n",
    "        lbl_dest_path = os.path.join(BASE, \"labels\", split_name, base_name + \".txt\")\n",
    "        if os.path.exists(lbl_src_path):\n",
    "            shutil.move(lbl_src_path, lbl_dest_path)\n",
    "\n",
    "        for ext in ['.jpg', '.jpeg', '.png']:\n",
    "            img_src_path = os.path.join(IMG_SRC_DIR, base_name + ext)\n",
    "            if os.path.exists(img_src_path):\n",
    "                img_dest_path = os.path.join(BASE, \"images\", split_name, base_name + ext)\n",
    "                shutil.move(img_src_path, img_dest_path)\n",
    "                break\n",
    "        # --- ë¡œì§ ì¶”ê°€ ë ---\n",
    "\n",
    "print(\"\\nğŸ‰ Iterative Stratificationì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(\"-\" * 30)\n",
    "# train í´ë”ì— ë‚¨ì€ íŒŒì¼ ìˆ˜ë¥¼ ì„¸ë„ë¡ ìˆ˜ì •\n",
    "print(f\"  - í›ˆë ¨(train) ì„¸íŠ¸: {len(os.listdir(IMG_SRC_DIR))}ê°œ\")\n",
    "print(f\"  - ê²€ì¦(val) ì„¸íŠ¸: {len(val_files)}ê°œ\")\n",
    "print(f\"  - í…ŒìŠ¤íŠ¸(test) ì„¸íŠ¸: {len(test_files)}ê°œ\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2005a34a0bbcc7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:05:40.275193Z",
     "start_time": "2025-08-25T15:05:40.125912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê° ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...\n",
      "\n",
      "--- [TRAIN ì„¸íŠ¸] ---\n",
      "   Class  Count\n",
      "0      0     65\n",
      "1      1    192\n",
      "2      2     69\n",
      "3      3     68\n",
      "4      4     67\n",
      "5      5     70\n",
      "6      6     70\n",
      "\n",
      "--- [VAL ì„¸íŠ¸] ---\n",
      "   Class  Count\n",
      "0      0      9\n",
      "1      1     19\n",
      "2      2     10\n",
      "3      3     10\n",
      "4      4     10\n",
      "5      5     10\n",
      "6      6     10\n",
      "\n",
      "--- [TEST ì„¸íŠ¸] ---\n",
      "   Class  Count\n",
      "0      0     18\n",
      "1      1     44\n",
      "2      2     19\n",
      "3      3     19\n",
      "4      4     19\n",
      "5      5     20\n",
      "6      6     20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# --- ì„¤ì • ---\n",
    "BASE = r\"./object_detection_\"\n",
    "# --- ì„¤ì • ---\n",
    "\n",
    "SPLITS_TO_CHECK = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "def analyze_split_distribution(split_name):\n",
    "    \"\"\"ì§€ì •ëœ ì„¸íŠ¸(train, val, test)ì˜ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    lbl_dir = os.path.join(BASE, \"labels\", split_name)\n",
    "\n",
    "    if not os.path.isdir(lbl_dir):\n",
    "        print(f\"\\n--- [{split_name.upper()} ì„¸íŠ¸] ---\")\n",
    "        print(f\"âŒ ì˜¤ë¥˜: '{lbl_dir}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    class_ids = []\n",
    "    for filename in os.listdir(lbl_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(lbl_dir, filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if parts:\n",
    "                        class_ids.append(int(float(parts[0])))\n",
    "\n",
    "    print(f\"\\n--- [{split_name.upper()} ì„¸íŠ¸] ---\")\n",
    "    if not class_ids:\n",
    "        print(\"ë¶„ì„í•  ë¼ë²¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    class_counts = Counter(class_ids)\n",
    "    df = pd.DataFrame(class_counts.items(), columns=['Class', 'Count'])\n",
    "    df_sorted = df.sort_values(by='Class').reset_index(drop=True) # í´ë˜ìŠ¤ ID ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "\n",
    "    print(df_sorted.to_string())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"ê° ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤...\")\n",
    "    for split in SPLITS_TO_CHECK:\n",
    "        analyze_split_distribution(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f31511893579c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-26T00:45:34.285276Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data.yaml ì´ë¯¸ ì¡´ì¬: ./object_detection_\\data.yaml\n",
      "[OK] ë¼ë²¨ ë§¤ì¹­ ì´ìƒ ì—†ìŒ\n",
      "[INFO] Loading model: ./yolov8n.pt\n",
      "[INFO] Start training...\n",
      "New https://pypi.org/project/ultralytics/8.3.186 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.176  Python-3.11.2 torch-2.7.1+cpu CPU (AMD Ryzen 5 5600U with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./object_detection_\\data.yaml, degrees=5, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.15, mode=train, model=./yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp_car_yolov8n, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=./object_detection_/runs_yolo_bok, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=object_detection_\\runs_yolo_bok\\exp_car_yolov8n, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.1, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.05, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=7\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752677  ultralytics.nn.modules.head.Detect           [7, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,213 parameters, 3,012,197 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 433.2197.8 MB/s, size: 217.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train... 471 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471/471 [00:00<00:00, 814.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 791.6264.0 MB/s, size: 217.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\val... 69 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:00<00:00, 1024.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\wjbok\\Desktop\\BigTech\\object_detection_\\labels\\val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\wjbok\\Desktop\\BigTech\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to object_detection_\\runs_yolo_bok\\exp_car_yolov8n\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000909, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mobject_detection_\\runs_yolo_bok\\exp_car_yolov8n\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100         0G      1.489      4.698      1.258         43        640:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [01:35<01:13,  5.67s/it]"
     ]
    }
   ],
   "source": [
    "# train_yolo.py\n",
    "# -----------------------------\n",
    "# Ultralytics YOLO í•™ìŠµ ë° PTQ ì ìš© ìŠ¤í¬ë¦½íŠ¸\n",
    "# -----------------------------\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ========= ì‚¬ìš©ì ì„¤ì • =========\n",
    "DATASET_DIR = r\"./object_detection_\"\n",
    "PROJECT_DIR = r\"./object_detection_/runs_yolo_bok\"\n",
    "EXP_NAME    = \"exp_car_yolov8n\"\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"animal\", \"person\", \"traffic_red\",\"traffic_yellow\", \"traffic_green\", \"right\", \"left\"\n",
    "]\n",
    "\n",
    "MODEL_NAME = \"./yolov8n.pt\"\n",
    "EPOCHS     = 100\n",
    "IMGSZ      = 320\n",
    "BATCH      = 16\n",
    "LR0        = 0.005\n",
    "PATIENCE   = 10\n",
    "DEVICE     = \"cpu\"\n",
    "\n",
    "DO_PREDICT_SAMPLES = True\n",
    "DO_EXPORT_OPENVINO = True # PTQë¥¼ ì ìš©í•  ê²ƒì´ë¯€ë¡œ Trueë¡œ ìœ ì§€\n",
    "DO_EXPORT_ONNX     = False\n",
    "\n",
    "PREDICT_SOURCE = os.path.join(DATASET_DIR, \"images\", \"val\")\n",
    "PREDICT_CONF   = 0.25\n",
    "\n",
    "# ========= ìœ í‹¸ =========\n",
    "def ensure_yaml(dataset_dir, class_names):\n",
    "    \"\"\"data.yaml ìë™ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ë®ì–´ì“°ì§€ ì•ŠìŒ)\"\"\"\n",
    "    yaml_path = os.path.join(dataset_dir, \"data.yaml\")\n",
    "    if os.path.exists(yaml_path):\n",
    "        print(f\"[INFO] data.yaml ì´ë¯¸ ì¡´ì¬: {yaml_path}\")\n",
    "        return yaml_path\n",
    "\n",
    "    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ yaml íŒŒì¼ì— ê¸°ë¡\n",
    "    abs_dataset_dir = os.path.abspath(dataset_dir)\n",
    "    content = [\n",
    "        f\"path: {abs_dataset_dir}\", # ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©\n",
    "        \"train: images/train\",\n",
    "        \"val: images/val\",\n",
    "        \"test: images/test\",\n",
    "        \"names:\"\n",
    "    ]\n",
    "    for i, name in enumerate(class_names):\n",
    "        content.append(f\"  {i}: {name}\")\n",
    "\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(content) + \"\\n\")\n",
    "\n",
    "    print(f\"[OK] data.yaml ìƒì„±: {yaml_path}\")\n",
    "    return yaml_path\n",
    "\n",
    "\n",
    "def sanity_check(dataset_dir):\n",
    "    \"\"\"ê°„ë‹¨ ë¬´ê²°ì„± ì²´í¬\"\"\"\n",
    "    img_train_dir = os.path.join(dataset_dir, \"images\", \"train\")\n",
    "    lbl_train_dir = os.path.join(dataset_dir, \"labels\", \"train\")\n",
    "    if not os.path.isdir(img_train_dir) or not os.path.isdir(lbl_train_dir):\n",
    "        print(f\"[WARN] train í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ sanity checkë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    missing = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "    for f in os.listdir(img_train_dir):\n",
    "        ext = os.path.splitext(f)[1].lower()\n",
    "        if ext not in exts: continue\n",
    "        stem = os.path.splitext(f)[0]\n",
    "        if not os.path.exists(os.path.join(lbl_train_dir, stem + \".txt\")):\n",
    "            missing.append(f)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"[WARN] ë¼ë²¨ ëˆ„ë½ ì´ë¯¸ì§€ {len(missing)}ê°œ ì˜ˆì‹œ: {missing[:10]}\")\n",
    "    else:\n",
    "        print(\"[OK] ë¼ë²¨ ë§¤ì¹­ ì´ìƒ ì—†ìŒ\")\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 1) data.yaml ë³´ì¥\n",
    "    data_yaml = ensure_yaml(DATASET_DIR, CLASS_NAMES)\n",
    "\n",
    "    # 2) ê°„ë‹¨ ì²´í¬\n",
    "    sanity_check(DATASET_DIR)\n",
    "\n",
    "    # 3) ëª¨ë¸ ë¡œë“œ\n",
    "    print(f\"[INFO] Loading model: {MODEL_NAME}\")\n",
    "    model = YOLO(MODEL_NAME)\n",
    "\n",
    "    # 4) í•™ìŠµ\n",
    "    print(\"[INFO] Start training...\")\n",
    "    results = model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=EPOCHS,\n",
    "        imgsz=IMGSZ,\n",
    "        batch=BATCH,\n",
    "        device=DEVICE,\n",
    "        project=PROJECT_DIR,\n",
    "        name=EXP_NAME,\n",
    "        lr0=LR0,\n",
    "        patience=PATIENCE,\n",
    "        optimizer=\"auto\",\n",
    "        hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,\n",
    "        fliplr=0.5,\n",
    "        mosaic=1.0, mixup=0.15,\n",
    "        degrees=5, translate=0.05, scale=0.1, shear=0.0, perspective=0.0,\n",
    "    )\n",
    "\n",
    "    run_dir = results.save_dir\n",
    "    best_pt = os.path.join(run_dir, \"weights\", \"best.pt\")\n",
    "    print(f\"[OK] Training done. best: {best_pt}\")\n",
    "\n",
    "    # 5) ê²€ì¦(mAP, PRì»¤ë¸Œ)\n",
    "    print(\"[INFO] Validate best weights...\")\n",
    "    model = YOLO(best_pt)\n",
    "    model.val(data=data_yaml, project=PROJECT_DIR, name=f\"{EXP_NAME}_val\")\n",
    "\n",
    "    # 6) ì˜ˆì¸¡ ìƒ˜í”Œ ì €ì¥\n",
    "    if DO_PREDICT_SAMPLES and os.path.exists(PREDICT_SOURCE):\n",
    "        print(f\"[INFO] Predict & save samples from: {PREDICT_SOURCE}\")\n",
    "        model.predict(\n",
    "            source=PREDICT_SOURCE,\n",
    "            conf=PREDICT_CONF,\n",
    "            save=True,\n",
    "            project=PROJECT_DIR,\n",
    "            name=f\"{EXP_NAME}_pred_val\"\n",
    "        )\n",
    "\n",
    "     # 7) ë‚´ë³´ë‚´ê¸° (PTQ ì ìš©)\n",
    "    if DO_EXPORT_OPENVINO:\n",
    "        print(\"[INFO] Export OpenVINO IR with Data-aware INT8 Quantization (PTQ)...\")\n",
    "\n",
    "        # âœ… 'data'ì™€ 'imgsz'ë¥¼ ëª…ì‹œí•˜ì—¬ ìš°ë¦¬ ë°ì´í„°ì…‹ì— ë§ê²Œ ì–‘ìí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "        model.export(\n",
    "            format=\"openvino\",    # OpenVINO í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°\n",
    "            int8=True,            # INT8 ì–‘ìí™” í™œì„±í™”\n",
    "            data=data_yaml,       # êµì • ë°ì´í„°ë¡œ ìš°ë¦¬ val ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì§€ì •\n",
    "            imgsz=IMGSZ,          # í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ ì´ë¯¸ì§€ í¬ê¸°ë¡œ êµì •\n",
    "            half=False,           # FP16 ëŒ€ì‹  INT8ì„ ëª©í‘œë¡œ í•˜ë¯€ë¡œ False\n",
    "            simplify=True         # ONNX ëª¨ë¸ì„ ë‹¨ìˆœí™”í•˜ì—¬ í˜¸í™˜ì„± ë° ì†ë„ í–¥ìƒ\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"[DONE] All finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97dcfc1c6e02415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861836d339b609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd48da4823fd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b705bffa7e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74d82d81766055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804b5f730351b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
